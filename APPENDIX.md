| <nobr>ç¼–å·</nobr> | å‘ç°å†…å®¹ | ç±»å‹ |
|------|---------|------|
| 1 | As the dimensionality increases, the performance advantages of all DCO optimization methods progressively improve. The projection pruning-based method can replace exact distances with distances computed in lower dimensions, while the quantization method accelerates each distance calculation. | <nobr>âœ… å°è¯å‰æ–‡ </nobr>|
| 2 | When the data scale is small, the computational time saved is insufficient to offset the additional overhead resulting from fewer DCO executions. As the data scale increases, the number of negative objects increases substantially, making the effect of DCO optimizations more significant. | <nobr><nobr>ğŸ†• æ–°å‘ç°</nobr></nobr> |
| 3 | The search process of HNSW is path-dependent, so erroneous pruning by DCO may disrupt the search path. The PCA-based projection method is more prone to erroneous pruning during the early stages of search, resulting in the need to explore a larger range to achieve the same recall rate. | <nobr><nobr>ğŸ†• æ–°å‘ç°</nobr></nobr> |
| 4 | Random projection evens out dimensional variance, yet its proportion-based distance estimation stays conservative. DADE's estimation depends on pre-trained global distribution arrays, often leading to errors with atypical queries. DDC_res improves on this by integrating query-specific data, resulting in better stability. Learning-based methods effectively learn decision boundaries for fast pruning and offer good generalizability. While Flash suffers from unsatisfactory recall due to over-compression, RaBitQ provides a balance of efficient quantization and accurate distance estimation. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 5 | Due to dimensional pruning, projection pruning-based DCO derives limited gains from SIMD acceleration. The efficiency of SIMD instructions is inherently contingent upon a branchless, linear data flow. However, the hypothesis testing in projection sampling disrupts this execution pattern, leading to a significant surge in branch mispredictions. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 6 | The performance advantages of projection pruning remain significant when the overhead of branch mispredictions is amortized over a large volume of DCO operations. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 7 | When DCOs become the bottleneck, DCO optimizations become more critical, as the overhead of these optimizations does not negate the overall acceleration gains. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 8 | Enabling automatic vectorization through data layout optimization enhances the pruning power of projection pruning methods and reduces the costs associated with hypothesis testing.| <nobr>âœ… å°è¯å‰æ–‡ </nobr> |
| 9 | Integrating HNSW with quantization lookup tables exacerbates its inherent issue of heavy random memory access. Furthermore, due to its path-dependent nature, HNSW cannot batch-parallelize quantized distance computations for large sets of vectors, instead requiring frequent interruptions to parallel execution. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 10 | RaBitQ achieves peak performance in SIMD execution across both index types, a benefit derived from its efficient quantized distance estimation and thorough co-design with the indexes. However, the scalar-simulated implementation of FastScan would offset the advantages of RaBitQâ€™s efficient quantization.| <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 11 | The memory overhead of projection pruning and quantization methods is negligible compared to the memory footprint of the index itself. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 12 | Most of the DCO optimizations are not standalone plugins; they require two layers of pre-processing: (1) an index-specific pre-computation phase to transform data and generate auxiliary structures, and (2) a per-query pre-processing step that itself consumes computational resources. Practitioners must prudently evaluate the overhead: whether the substantial cost of index pre-processing is justifiable for their application, and crucially, whether the latency added by query pre-processing might nullify acceleration benefits, ultimately degrading performance.| <nobr>âœ… å°è¯å‰æ–‡ </nobr> |
| 13 | Since achieving one recall in index optimization still encounters DCO bottlenecks, DCO optimization is paramount. Furthermore, the co-design of DCO optimizations and index structures is essential. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |
| 14 | Guidelines for selecting DCOs across various scenarios. | <nobr>ğŸ†• æ–°å‘ç°</nobr> |